### 1.5.1 分布式概述

es 支持集群模式，是一个分布式系统，其好处主要有两个：

* 增大系统容量，如内存、磁盘，使得es集群可以支持PB级的数据
* 提高系统可用性，即使部分节点停止服务，整个集群依然可以正常服务

es 集群由多个es实例组成

* 不同集群通过集群名字来区分，可通过clustername 进行修改，默认为elasticsearch
* 每个es实例本质上是一个JVM进程，且有自己的名字，通过node.name进行修改

Cerebro是一个可以查看es集群管理工具，地址：https://github.com/lmenezes/cerebro

### 1.5.2 构建集群

启动一个节点

运行如下命令可以启动一个es节点实例

`bin/elasticsearch -Eclustername=my_cluster -Enode.name=node1`

可以启动多个节点实例构建一个es集群

#### Cluster State

es 集群相关的数据称为cluster state，主要记录如下信息：

* 节点信息，比如节点名称、连接地址等
* 索引信息，比如索引名称、配置等
* ……

#### Master Node

* 可以修改cluster state的节点称为**master节点**，一个集群**只能有一个**
* cluster state 存储在每个节点上，master 维护最新版本并同步给其他节点
* master 节点是通过集群中所有节点选举产生的，可以被选举的节点称为 **master-eligible节点**，相关配置为：`-node.master:true`

#### Coordinating Node

* 处理请求的节点即为 **coordinating节点**，该节点是所有节点的默认角色，不能取消
* 作用是路由请求到正确的节点处理，比如创建索引的请求到master节点

#### Data Node

存储数据的节点即为 **data节点**，默认节点都是data类型，相关配置为：`node.data:true`

### 1.5.3 副本与分片

#### 副本

提高系统可用性，包括以下两个方面

* 服务可用性：2个节点的情况下，允许其中1个节点停止服务
* 数据可用性：引入**副本（Replication）**解决，每个节点上都有完备的数据

#### 分片

如何将数据分布于所有节点上？es通过引入**分片（Shard**）解决问题

* 分片是es支持PB级数据的基石
* 分片存储了部分数据，可以分布于任意节点上
* 分片数在索引创建时指定且后续不允许再更改，默认为5个
* 分片有主分片和副本分片之分，以实现数据的高可用
* 副本分片的数据由主分片同步，可以有多个，从而提高读取的吞吐量

下图演示的是3个节点的集群中test_index的分片分布情况，创建时我们指定了3个分片和1个副本，api如下所示：

![image-20201226233241250](https://s3.ax1x.com/2020/12/29/r7cArd.png)

#### 副本与分片的思考

基于上图，思考两个问题：

* 问：此时增加节点是否能提高test_index的数据容量？

  答：不能。因为只有3个分片，已经分布在3台节点上，新增的节点无法利用。

  ![image-20201226233421953](https://s3.ax1x.com/2020/12/29/r7ckKH.png)

* 问：此时增加副本数是否能提高test_index的读取吞吐量？

  答：不能。因为新增的副本也是分布在这3个节点上，还是利用了同样的资源。如果要增加吞吐量，还需要新增节点。

  ![image-20201226233527667](https://s3.ax1x.com/2020/12/29/r7cEqA.png)

总结：分片数的设定很重要，需要提前规划好

* 过小会导致后续无法通过增加节点实现水平扩容
* 过大会导致一个节点上分布过多分片，造成资源浪费，同时会影响查询性能

### 1.5.4 集群状态

#### 健康状态

通过如下api可以查看集群健康状况，包括以下三种：

```
GET _cluster/health
```

* green：健康状态，指所有主副分片都正常分配
* yellow：指所有主分片都正常分配，但是有副本分片未正常分配
* red：有主分片未分配（比如磁盘空间不足）

集群处于red状态并不意味着不能对外提供服务，是可以正常访问的

#### 故障转移

集群由3个节点组成，如下所示，此时集群状态是green

![image-20201226234606149](https://s3.ax1x.com/2020/12/29/r7cKG8.png)

node1所在机器宕机导致服务终止，此时集群会如何处理？

1. node之间会互相ping检查节点状态，node2和node3发现node1无法响应一段时间后，会发起master选举，比如这里选择node2为master节点。此时由于主分片P0下线，集群状态变为Red。

   ![image-20201226234632847](https://s3.ax1x.com/2020/12/29/r7cuPf.png)

2. node2发现主分片P0未分配，将R0提升为主分片。此时由于所有主分片都正常分配，集群状态变为Yellow。

   ![image-20201226234957010](https://s3.ax1x.com/2020/12/29/r7cmIP.png)

3. node2为P0和P1生成新的副本，集群状态变为绿色。

   ![image-20201226235017175](https://s3.ax1x.com/2020/12/29/r7ceat.png)

### 1.5.5 文档分布式存储

文档最终会存储在分片上。如下图所示，Document1最终存储在分片P1上

![image-20201226235606967](https://s3.ax1x.com/2020/12/29/r7cZVI.png)

Document1是如何存储到分片P1的？选择P1的依据是什么？

答案：有一个文档到分片的映射算法

目的：使得文档均匀分布在所有分片上，以充分利用资源

算法：是否可以随机选择或者round-robin算法？不可取，因为需要维护文档到分片的映射关系，成本巨大。应该根据文档值实时计算对应的分片

**映射算法：**

es通过如下的公式计算文档对应的分片

* shard=hash(routing)% number of primary shards-hash
* 算法保证可以将数据均匀地分散在分片中
* routing是一个关键参数，默认是文档id，也可以自行指定
* number_of _primary_shards是主分片数
* 该算法与主分片数相关，这也是**分片数一旦确定后便不能更改**的原因

**文档创建流程：**

![image-20201227001110197](https://s3.ax1x.com/2020/12/29/r7c8qs.png)

**文档读取流程：**

![image-20201227001025827](https://s3.ax1x.com/2020/12/29/r7c3rj.png)

**文档批量创建流程：**

![image-20201227001317524](https://s3.ax1x.com/2020/12/29/r7c1MQ.png)

**文档批量读取流程：**

![image-20201227001439884](https://s3.ax1x.com/2020/12/29/r7cQxg.png)

### 1.5.6 脑裂问题

脑裂问题，英文为split-brain，是分布式系统中的经典网络问题，如下图所示：

* node2与node3会重新选举master，比如node2成为了新master，此时会更新cluster state
* node1自己组成集群后，也会更新cluster state
* 同一个集群有两个master，而且维护不同的cluster state，网络恢复后无法选择正确的master

![image-20201227001547075](https://s3.ax1x.com/2020/12/29/r7ctI0.png)

解决方案为仅在可选举master-eligible 节点数大于等于quorum时才可以进行master选举

* quorum=master-eligible 节点数/2+1，例如3个master-eligible节点时， quorum为2
* 设定 **discovery.zen.minimum_master nodes** 为 quorum 即可避免脑裂

![image-20201227001802838](https://s3.ax1x.com/2020/12/29/r7cYaq.png)

### 1.5.7 shard详解

#### 索引不可变

**倒排索引一旦生成，不能更改**，其好处如下：

* 不用考虑并发写文件的问题，杜绝了锁机制带来的性能问题
* 由于文件不再更改，可以充分利用文件系统缓存，只需载入一次，只要内存足够，对该文件的读取都会从内存读取，性能高
* 利于生成缓存数据
* 利于对文件进行压缩存储，节省磁盘和内存存储空间

坏处为需要写入新文档时，必须重新构建倒排索引文件，然后替换老文件后，新文档才能被检索，导致文档实时性差。查询流程如下图：

![image-20201227025225189](https://s3.ax1x.com/2020/12/29/r7caGT.png)

#### 文档搜索实时性介绍

如何解决文档搜索实时性问题呢？

解决方案是新文档直接生成新的倒排索引文件，查询的时候同时查询所有的倒排文件，然后做结果的汇总计算即可。查询流程如下图：

![image-20201227025447071](https://s3.ax1x.com/2020/12/29/r7cUiV.png)

Lucene 便是采用了这种方案，它构建的单个倒排索引称为**segment**，合在一起称为**Index**，与ES中的Index概念不同。ES中的一个**Shard** 对应一个**Lucene Index**。
Lucene 会有一个专门的文件来记录所有的**segment**信息，称为**commit point**

![image-20201227025633407](https://s3.ax1x.com/2020/12/29/r7cwzF.png)

#### 文档搜索实时性—refresh

* **segment** 写入磁盘的过程依然很耗时，可以 借助文件系统缓存的特性，先将 **segment**在缓存中创建并开放查询来进一步提升实时性，该过程在es中被称为**refresh**。
* 在**refresh**之前文档会先存储在一个buffer中，**refresh**时将buffer中的所有文档清空并生成segment
* es默认每1秒执行一次refresh，因此文档的实时性被提高到1秒，这也是es被称为近实时（Near Real Time)的原因

![image-20201227030400157](https://s3.ax1x.com/2020/12/29/r7cdRU.png)

#### 文档搜索实时性—translog

如果在内存中的 segment还没有写入磁盘前发生了宕机，那么其中的文档就无法恢复了，如何解决这个问题？

* es引入**translog** 机制。写入文档到**buffer**时，同时将该操作写入**translog**。
* **translog** 文件会即时写入磁盘（**fsync**)，6.x默认每个请求都会落盘，可以修改为每5秒写一次，这样风险便是丢失5秒内的数据，相关配置为 **index.translog.***
* es 启动时会检查**translog**文件，并从中恢复数据

![image-20201227030711001](https://s3.ax1x.com/2020/12/29/r7cBM4.png)

#### 文档搜索实时性—flush

flush负责将内存中的segment写入磁盘，主要做如下的工作：

* 将translog 写入磁盘
* 将index buffer 清空，其中的文档生成一个新的segment，相当于一个refresh操作
* 更新commit point 并写入磁盘
* 执行fsync操作，将内存中的segment写入磁盘
* 删除旧的translog文件

![image-20201227030914473](https://s3.ax1x.com/2020/12/29/rqndat.png)

#### 实时性操作发生时机

**refresh 发生的时机**主要有如下几种情况：

* 间隔时间达到时，通过**index.settings.refresh interval**来设定，默认是1秒
* index.buffer 占满时，其大小通过**indices.memory.index_buffer_size**设置，默认为jvm heap的10%，所有shard共享
* flush 发生时也会发生refresh

**flush 发生的时机**主要有如下几种情况：

* 间隔时间达到时，默认是30分钟，5.x之前可以通过**index.translog.flush threshold period**修改，之后版本无法修改
* translog 占满时，其大小可以通过**index.translog.flush threshold _size** 控制，默认是512mb，每个index有自己的translog

#### 删除与更新文档

**文档搜索实时性—删除与更新文档**

* segment一旦生成就不能更改，那么如果你要删除文档该如何操作？

  Lucene 专门维护一个.del的文件，记录所有已经删除的文档，注意.del上记录的是文档在Lucene内部的id-在查询结果返回前会过滤掉.del中的所有文档

* 更新文档如何进行呢？

  首先删除文档，然后再创建新文档

#### 整体视角及 Segment Merging

**整体视角**

ES Index与Lucene Index的术语对照如下所示：

![image-20201227031538628](https://s3.ax1x.com/2020/12/29/rqnHsJ.png)

**Segment Merging**

* 随着 segment的增多，由于一次查询的segment数增多，查询速度会变慢
* es会定时在后台进行 segment merge的操作，减少segment的数量
* 通过force_merge api可以手动强制做segment merge的操作
